{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from unisim import TextSim\n",
    "from tabulate import tabulate\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "For this example, we use a product matching dataset \n",
    "\n",
    "https://huggingface.co/datasets/RUC-DataLab/ER-dataset\n",
    "\n",
    "https://github.com/ruc-datalab/DADER\n",
    "\n",
    "There are additional datasets that test entity retrieval and matching\n",
    "\n",
    "citation, product matching, restaurant matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"RUC-DataLab/ER-dataset\", data_files=\"dblp_scholar.csv\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restaurants, products are good to do like differences in matching addresses etc easy to do \n",
    "\n",
    "# restaurants1 not bad, better than fodors zagats which gets 100 percent\n",
    "# beer is pretty good\n",
    "# walmart amazon not bad\n",
    "# itunes_amazon is meh maybe too easy\n",
    "# dblp_sholcar and acm decent might be too easy , scholar is harder than ACM which is like 99% accuracy\n",
    "# restauratnts 3 and 4 are decent but might be too easy\n",
    "# fodors zagats too aesy\n",
    "\n",
    "# not good because book editions books 4 > book 2\n",
    "# anime not easy either its like the diference is saesons \n",
    "# cosmetics not good either, it's like different colors are not clasified as the same thing\n",
    "# abt_buy not that good, missing descriptions and not super clean it seems \n",
    "# movies1 not good year range is kinda weird/ \n",
    "# shoes and comuters not great, there are near-dups that are marked as not the same item \n",
    "# ebboks not good, based on similarity description similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A_title': ['the demarcation protocol : a technique for maintaining constraints in distributed database systems ',\n",
       "  'on-demand data elevation in hierarchical multimedia storage servers ',\n",
       "  'database tuning : principles , experiments , and troubleshooting techniques ',\n",
       "  'dna-miner : a system prototype for mining dna sequences ',\n",
       "  'communication efficient distributed mining of association rules '],\n",
       " 'A_authors': ['d barbar혜 , h garcia-molina ',\n",
       "  'p triantafillou , t papadakis ',\n",
       "  'd shasha , p bonnet ',\n",
       "  'j han , h jamil , y lu , l chen , y liao , j pei ',\n",
       "  'a schuster , r wolff '],\n",
       " 'A_venue': ['vldb j. ',\n",
       "  'vldb ',\n",
       "  'vldb ',\n",
       "  'sigmod conference ',\n",
       "  'sigmod conference '],\n",
       " 'A_year': [1994, 1997, 2002, 2001, 2001],\n",
       " 'B_title': ['local verification of global integrity constraints in distributed databases ',\n",
       "  'on-demand data elevation in a hierarchical multimedia storage server ',\n",
       "  'database tuning : principles , experiments , and troubleshooting techniques ( part i ) ',\n",
       "  'n. stefanovic 1997 칙 ?? geominer : a system prototype for spatial data mining칙 ?? ',\n",
       "  'mining generalized association rules '],\n",
       " 'B_authors': ['a gupta , j widom ',\n",
       "  'p triantallou , t papadakis ',\n",
       "  'd shasha , p bonnet ',\n",
       "  'j han , k koperski ',\n",
       "  'r agrawal , r srikant '],\n",
       " 'B_venue': [' ',\n",
       "  'proc . of 23rd intl. conf . on very large data bases , vldb , ',\n",
       "  'proceedings of the 2002 acm sigmod international conference & hellip ; , ',\n",
       "  'proc . acm-sigmod int . conf . on management of data ( sigmod &#39; 97 ',\n",
       "  'proceedings of the 1995 international conference of very & hellip ; , '],\n",
       " 'B_year': [' ', '1997.0 ', '2002.0 ', ' ', ' '],\n",
       " 'label': [0, 1, 1, 0, 0]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 28707\n",
      "Dataset features: ['A_title', 'A_authors', 'A_venue', 'A_year', 'B_title', 'B_authors', 'B_venue', 'B_year', 'label']\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of dataset:\", len(dataset))\n",
    "\n",
    "dataset_features = list(dataset.features.keys())\n",
    "print(\"Dataset features:\", dataset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_features = [x for x in dataset_features if x.startswith(\"A\")]\n",
    "text2_features = [x for x in dataset_features if x.startswith(\"B\")]\n",
    "is_match_feature = \"label\"\n",
    "\n",
    "def get_text_pair(idx):\n",
    "    ex = dataset[idx]\n",
    "\n",
    "    text1 = \" \".join(str(ex[x]) for x in text1_features)\n",
    "    text2 = \" \".join(str(ex[x]) for x in text2_features)\n",
    "\n",
    "    label = ex[is_match_feature]\n",
    "    return [text1, text2, label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "INFO: UniSim is storing a copy of the indexed data\n",
      "INFO: If you are using large data corpus, consider disabling this behavior using store_data=False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4220390021800995"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sim = TextSim(index_type=\"exact\")\n",
    "text_sim.similarity(\"this is a text\", \"apples\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Similarity between Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9418152570724487"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sim.similarity(\"this is an example text\", \"This is an example txt! 游 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5612893104553223"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sim.similarity(\"hello\", \"h3110\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      query     target  similarity  is_match\n",
      "0     apple      apple    1.000000      True\n",
      "1      appl      apple    0.914230      True\n",
      "2  icecream  ice cream    0.950734      True\n",
      "3     house      mouse    0.760066     False\n",
      "4    random      mouse    0.456315     False\n"
     ]
    }
   ],
   "source": [
    "queries = [\"apple\", \"appl\", \"icecream\", \"house\", \"random\"]\n",
    "targets = [\"apple\", \"ice cream\", \"mouse\"]\n",
    "\n",
    "results_df = text_sim.match(queries, targets, similarity_threshold=0.9)\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 256)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can directly compute text embeddings using TextSim's `embed()` method\n",
    "example_texts = [\n",
    "    \"This is an example text!\",\n",
    "    \"You can even use extremely long texts, TextSim is capable of handling and matching arbitrarily-long texts.\"\n",
    "]\n",
    "embeddings = text_sim.embed(example_texts)\n",
    "embeddings.shape  # (2, 256) dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "INFO: UniSim is storing a copy of the indexed data\n",
      "INFO: If you are using large data corpus, consider disabling this behavior using store_data=False\n"
     ]
    }
   ],
   "source": [
    "# you can maintain an index of texts as well. \n",
    "# we support Approximate Nearest Neighbor search, with index_type=\"approx\" using USearch, which will scale to millions or even billions of indexed examples\n",
    "\n",
    "text_sim = TextSim()\n",
    "\n",
    "# dataset\n",
    "index_examples = [\n",
    "    \"I love ice cream and cookies\",\n",
    "    \"Ice cream is super delicious\",\n",
    "    \"my mom makes the best homemade cookies 游꼵游꼵游꼵\",\n",
    "    \"This is an example text.\",\n",
    "    \"UniSim supports very long texts as well.\",\n",
    "    \"UniSim supports multilingual texts too. 擔먼봏!\",\n",
    "]\n",
    "text_sim.add(index_examples)\n",
    "\n",
    "# you want to find nearest things in your index\n",
    "query_examples = [\n",
    "    \"I luv ice cream and cookies游꼱游꼵\",\n",
    "    \"This is an example query text.\",\n",
    "    \"Unrelated text with no match in the dataset...\"\n",
    "]\n",
    "\n",
    "result_collection = text_sim.search(query_examples, similarity_threshold=0.9, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_matches = result_collection.total_matches\n",
    "total_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0: \"I luv ice cream and cookies游꼱游꼵\"\n",
      "Most similar matches:\n",
      "\n",
      "  idx  is_match      similarity  text\n",
      "-----  ----------  ------------  ---------------------------------------------\n",
      "    0  True                0.91  I love ice cream and cookies\n",
      "    1  False               0.66  Ice cream is super delicious\n",
      "    2  False               0.53  my mom makes the best homemade cookies 游꼵游꼵游꼵\n",
      "    3  False               0.42  This is an example text.\n",
      "    4  False               0.36  UniSim supports very long texts as well.\n"
     ]
    }
   ],
   "source": [
    "result = result_collection.results[0]\n",
    "text_sim.visualize(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(result_collection.total_matches)\n",
    "results = result_collection.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_collection = text_sim.search([\"This is some text??\"], similarity_threshold=0.9, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0: \"This is some text??\"\n",
      "Most similar matches:\n",
      "\n",
      "  idx  is_match      similarity  text\n",
      "-----  ----------  ------------  ---------------------------------------------\n",
      "    3  False               0.74  This is an example text.\n",
      "    5  False               0.54  UniSim supports multilingual texts too. 擔먼봏!\n",
      "    4  False               0.51  UniSim supports very long texts as well.\n",
      "    1  False               0.45  Ice cream is super delicious\n",
      "    2  False               0.43  my mom makes the best homemade cookies 游꼵游꼵游꼵\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_sim.visualize(result_collection.results[0])\n",
    "\n",
    "# text_sim.visualize(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1                                                                                                                                           text2                                                                                                                                                                                            is_match    similarity\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------  ------------\n",
      "the demarcation protocol : a technique for maintaining constraints in distributed database systems  d barbar혜 , h garcia-molina  vldb j.  1994  local verification of global integrity constraints in distributed databases  a gupta , j widom                                                                                                          0      0.491886\n",
      "on-demand data elevation in hierarchical multimedia storage servers  p triantafillou , t papadakis  vldb  1997                                  on-demand data elevation in a hierarchical multimedia storage server  p triantallou , t papadakis  proc . of 23rd intl. conf . on very large data bases , vldb ,  1997.0                                1      0.837665\n",
      "database tuning : principles , experiments , and troubleshooting techniques  d shasha , p bonnet  vldb  2002                                    database tuning : principles , experiments , and troubleshooting techniques ( part i )  d shasha , p bonnet  proceedings of the 2002 acm sigmod international conference & hellip ; ,  2002.0           1      0.820263\n",
      "dna-miner : a system prototype for mining dna sequences  j han , h jamil , y lu , l chen , y liao , j pei  sigmod conference  2001              n. stefanovic 1997 칙 ?? geominer : a system prototype for spatial data mining칙 ??  j han , k koperski  proc . acm-sigmod int . conf . on management of data ( sigmod &#39; 97                           0      0.555746\n",
      "communication efficient distributed mining of association rules  a schuster , r wolff  sigmod conference  2001                                  mining generalized association rules  r agrawal , r srikant  proceedings of the 1995 international conference of very & hellip ; ,                                                                      0      0.593165\n"
     ]
    }
   ],
   "source": [
    "example_data = [get_text_pair(idx) for idx in range(0, 5)]\n",
    "\n",
    "for i in range(len(example_data)):\n",
    "    text1, text2, is_match = example_data[i]\n",
    "    similarity = text_sim.similarity(text1, text2)\n",
    "    example_data[i].append(similarity)\n",
    "\n",
    "print(tabulate(example_data, headers=[\"text1\", \"text2\", \"is_match\", \"similarity\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querying multi-dimensional data indexed using the hilbert space-filling curve  j lawder , p king  sigmod record  2001\n",
      "energy and rate based mac protocol for wireless sensor networks  r kannan , r kalidindi , s iyengar , v kumar  sigmod record  2003\n",
      "query processing over object views of relational data  g fahl , t risch  vldb j.  1997\n",
      "message from editor-in-chief , acm transactions on database systems  w kim    1999\n",
      "the spiffi scalable video-on-demand system  c freedman , d dewitt  sigmod conference  1995\n"
     ]
    }
   ],
   "source": [
    "targets = list(set([get_text_pair(idx)[0] for idx in range(0, len(dataset))]))  # TODO does not preserve order \n",
    "print(\"\\n\".join([t for t in targets[:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "olap + + : powerful and easy-to-use federations of olap and object databases  j gu , tb pedersen , a shoshani  proceedings of the 26th international conference on very & hellip ; ,  2000.0 \n",
      "loading databases using dataflow parallelism  j gray칙 ? 춵  sigmod record ,   \n",
      "squeezer : an efficient algorithm for clustering categorical data  h zengyou , x xiaofei , d shengchun  journal of computer science and technology ,  2002.0 \n",
      "the federated data warehouse  c white     \n",
      "mining fuzzy association rules in databases .  acms anthology  sigmod record ,  1998.0 \n"
     ]
    }
   ],
   "source": [
    "queries = list(set([get_text_pair(idx)[1] for idx in range(0, len(dataset))]))\n",
    "print(\"\\n\".join([t for t in queries[:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>target</th>\n",
       "      <th>similarity</th>\n",
       "      <th>is_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>olap + + : powerful and easy-to-use federations of olap and object databases  j gu , tb pedersen , a shoshani  proceedings of the 26th international conference on very &amp; hellip ; ,  2000.0</td>\n",
       "      <td>olap + + : powerful and easy-to-use federations of olap and object databases  j gu , t pedersen , a shoshani  vldb  2000</td>\n",
       "      <td>0.876110</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loading databases using dataflow parallelism  j gray칙 ? 춵  sigmod record ,</td>\n",
       "      <td>loading databases using dataflow parallelism  t barclay , r barnes , j gray , p sundaresan  sigmod record  1994</td>\n",
       "      <td>0.817940</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>squeezer : an efficient algorithm for clustering categorical data  h zengyou , x xiaofei , d shengchun  journal of computer science and technology ,  2002.0</td>\n",
       "      <td>efficient and extensible algorithms for multi query optimization  p roy , s seshadri , s sudarshan , s bhobe  sigmod conference  2000</td>\n",
       "      <td>0.670530</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the federated data warehouse  c white</td>\n",
       "      <td>data warehouse configuration  d theodoratos , t sellis  vldb  1997</td>\n",
       "      <td>0.657057</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mining fuzzy association rules in databases .  acms anthology  sigmod record ,  1998.0</td>\n",
       "      <td>mining fuzzy association rules in databases  c kuok , a fu , m wong  sigmod record  1998</td>\n",
       "      <td>0.871261</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>querying multidimensional databases  l cabibbo , r torlone</td>\n",
       "      <td>optimizing multiple dimensional queries simultaneously in multidimensional databases  w liang , m orlowska , j yu  vldb j.  2000</td>\n",
       "      <td>0.701327</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adaptable query optimization and evaluation in temporal middleware  g slivinskas , cs jensen , rt snodgrass  sigmod conference ,  2001.0</td>\n",
       "      <td>adaptable query optimization and evaluation in temporal middleware  g slivinskas , c jensen , r snodgrass  sigmod conference  2001</td>\n",
       "      <td>0.976707</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>design principles for data-intensive web sites .  acms anthology  sigmod record ,  1999.0</td>\n",
       "      <td>design principles for data-intensive web sites  s ceri , p fraternali , s paraboschi  sigmod record  1999</td>\n",
       "      <td>0.844806</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>칙 ?? sim : a database system based on semantic model .  d jagannathan , rl guck , bl fritchman , jp thompson ,  proceedings of sigmod international conference on management</td>\n",
       "      <td>spartan : a model-based semantic compression system for massive data tables  s babu , m garofalakis , r rastogi  sigmod conference  2001</td>\n",
       "      <td>0.654835</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>database abstractions : aggregation and generalization  jm smith , dcp smith  acm transactions on database systems ,  1977.0</td>\n",
       "      <td>solving satisfiability and implication problems in database systems  s guo , w sun , m weiss  acm trans . database syst .  1996</td>\n",
       "      <td>0.762832</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                           query  \\\n",
       "0  olap + + : powerful and easy-to-use federations of olap and object databases  j gu , tb pedersen , a shoshani  proceedings of the 26th international conference on very & hellip ; ,  2000.0    \n",
       "1                                                                                                                  loading databases using dataflow parallelism  j gray칙 ? 춵  sigmod record ,      \n",
       "2                                  squeezer : an efficient algorithm for clustering categorical data  h zengyou , x xiaofei , d shengchun  journal of computer science and technology ,  2002.0    \n",
       "3                                                                                                                                                     the federated data warehouse  c white        \n",
       "4                                                                                                        mining fuzzy association rules in databases .  acms anthology  sigmod record ,  1998.0    \n",
       "5                                                                                                                                querying multidimensional databases  l cabibbo , r torlone        \n",
       "6                                                      adaptable query optimization and evaluation in temporal middleware  g slivinskas , cs jensen , rt snodgrass  sigmod conference ,  2001.0    \n",
       "7                                                                                                     design principles for data-intensive web sites .  acms anthology  sigmod record ,  1999.0    \n",
       "8                칙 ?? sim : a database system based on semantic model .  d jagannathan , rl guck , bl fritchman , jp thompson ,  proceedings of sigmod international conference on management      \n",
       "9                                                                  database abstractions : aggregation and generalization  jm smith , dcp smith  acm transactions on database systems ,  1977.0    \n",
       "\n",
       "                                                                                                                                     target  \\\n",
       "0                  olap + + : powerful and easy-to-use federations of olap and object databases  j gu , t pedersen , a shoshani  vldb  2000   \n",
       "1                           loading databases using dataflow parallelism  t barclay , r barnes , j gray , p sundaresan  sigmod record  1994   \n",
       "2     efficient and extensible algorithms for multi query optimization  p roy , s seshadri , s sudarshan , s bhobe  sigmod conference  2000   \n",
       "3                                                                        data warehouse configuration  d theodoratos , t sellis  vldb  1997   \n",
       "4                                                  mining fuzzy association rules in databases  c kuok , a fu , m wong  sigmod record  1998   \n",
       "5          optimizing multiple dimensional queries simultaneously in multidimensional databases  w liang , m orlowska , j yu  vldb j.  2000   \n",
       "6        adaptable query optimization and evaluation in temporal middleware  g slivinskas , c jensen , r snodgrass  sigmod conference  2001   \n",
       "7                                 design principles for data-intensive web sites  s ceri , p fraternali , s paraboschi  sigmod record  1999   \n",
       "8  spartan : a model-based semantic compression system for massive data tables  s babu , m garofalakis , r rastogi  sigmod conference  2001   \n",
       "9           solving satisfiability and implication problems in database systems  s guo , w sun , m weiss  acm trans . database syst .  1996   \n",
       "\n",
       "   similarity  is_match  \n",
       "0    0.876110     False  \n",
       "1    0.817940     False  \n",
       "2    0.670530     False  \n",
       "3    0.657057     False  \n",
       "4    0.871261     False  \n",
       "5    0.701327     False  \n",
       "6    0.976707      True  \n",
       "7    0.844806     False  \n",
       "8    0.654835     False  \n",
       "9    0.762832     False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = text_sim.match(queries, targets)\n",
    "\n",
    "\n",
    "#  TODO find the ones that do have matches in the dataset and look them up\n",
    "# TODO find a few that do not have matches and look them up --- will show that they have no match\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(results_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Querying Similar Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " ...]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sim.reset_index()\n",
    "\n",
    "# add list of texts to the index\n",
    "text_sim.add(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_collection = text_sim.search(queries, similarity_threshold=0.9, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0: \"olap + + : powerful and easy-to-use federations of olap and object databases  j gu , tb pedersen , a shoshani  proceedings of the 26th international conference on very & hellip ; ,  2000.0 \"\n",
      "Most similar matches:\n",
      "\n",
      "  idx  is_match      similarity  text\n",
      "-----  ----------  ------------  ----------------------------------------------------------------\n",
      " 1869  False               0.88  olap + + : powerful and easy-to-use federations of olap and obje\n",
      "  855  False               0.58  report on the first international conference on ontologies , dat\n",
      " 1284  False               0.57  proceedings of the 2000 acm sigmod international conference on m\n",
      " 1614  False               0.56  building scalable internet applications with oracle8i server  j\n",
      "  136  False               0.56  application servers and associated technologies      2002\n"
     ]
    }
   ],
   "source": [
    "text_sim.visualize(result_collection.results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2524]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_examples = [\"Googleplex (650) 253-0000 1600 Amphitheatre Parkway, Mountain View, CA 94043\"]\n",
    "text_sim.add(new_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0: \"googleplx 650-253-0000 1600 amphitheatre parkway, mountain view, ca 94043\"\n",
      "Most similar matches:\n",
      "\n",
      "  idx  is_match      similarity  text\n",
      "-----  ----------  ------------  ----------------------------------------------------------------\n",
      " 2524  True                0.92  Googleplex (650) 253-0000 1600 Amphitheatre Parkway, Mountain Vi\n"
     ]
    }
   ],
   "source": [
    "result_collection = text_sim.search([\"googleplx 650-253-0000 1600 amphitheatre parkway, mountain view, ca 94043\"])\n",
    "result = result_collection.results[0]\n",
    "\n",
    "text_sim.visualize(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
